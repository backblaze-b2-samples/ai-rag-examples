{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Backblaze B2 Retrieval-Augmented Generation (RAG) Demo\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) allows you to build on an off-the-shelf large language model (LLM), adding custom context that the model can use in interacting with a user. You can use RAG to implement chatbots that use your own proprietary data to answer questions, without that data leaking to the internet. \n",
    "\n",
    "This notebook walks you through loading PDF files from [Backblaze B2 Cloud Object Storage](https://www.backblaze.com/cloud-storage) into a [LangChain](https://python.langchain.com/v0.2/docs/introduction/) RAG app, then building a chatbot that can answer questions relating to the content of those PDF files. You'll use an open-source language model that you run locally, rather than an online API, ensuring that your data stays confidential.\n",
    "\n",
    "The code is based on the LangChain tutorial, [Build a Local RAG Application](https://python.langchain.com/v0.2/docs/tutorials/local_rag/).\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "First, install the required Python packages:"
   ],
   "id": "75a51ec771ee51d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T19:22:08.638942Z",
     "start_time": "2024-07-17T19:22:03.229014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --quiet -r requirements.txt\n",
    "\n",
    "# Restart the kernel so that it uses the new modules\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ],
   "id": "57a3b3958767b924",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need a Backblaze B2 Account, Bucket and Application Key, and some PDF files. Follow these instructions, as necessary:\n",
    "\n",
    "* [Create a Backblaze B2 Account](https://www.backblaze.com/sign-up/cloud-storage).\n",
    "* [Create a Backblaze B2 Bucket](https://www.backblaze.com/docs/cloud-storage-create-and-manage-buckets).\n",
    "* [Create an Application Key](https://www.backblaze.com/docs/cloud-storage-create-and-manage-app-keys#create-an-app-key) with access to the bucket you wish to use.\n",
    "\n",
    "Be sure to copy the application key as soon as you create it, as you will not be able to retrieve it later!"
   ],
   "id": "b401105aa7ab8e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upload PDF Files to Your Bucket\n",
    "\n",
    "You can use the Backblaze web UI, or any B2 or S3-compatible file management tool to [upload PDF files to your bucket](https://www.backblaze.com/docs/cloud-storage-upload-and-manage-files). It's useful to organize files by prefix (analogous to a folder or directory in a traditional filesystem); this example assumes the PDFs have the prefix `pdfs/` within the bucket.\n",
    "\n",
    "If you don't have any suitable PDF files to hand, you can [download the Backblaze B2 documentation PDF used in creating this tutorial](https://metadaddy-langchain-demo.s3.us-west-004.backblazeb2.com/pdfs/documentation.pdf) and upload it to your own bucket. "
   ],
   "id": "9a708475a5b58ccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Since Backblaze B2 has an S3-compatible API, this notebook uses LangChain's [`S3FileLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html) and the [`s3fs`](https://s3fs.readthedocs.io/en/latest/) module to interact with files in Backblaze B2, as well as the AWS SDK for Python, also known as Boto3. Both `S3FileLoader` and `s3fs` use Boto3 under the covers, so you need simply configure the latter so that all of the tools can access your Backblaze B2 Bucket. The most straightforward way to do so in this context is via environment variables.\n",
    "\n",
    "Note: you should never, *ever* put credentials in your code, including Jupyter notebooks! This example uses `python-dotenv` to load configuration from a `.env` file into environment variables for use by `S3FileLoader`. This repo includes a template file, `.env.template`. Copy it to `.env`, then edit it as follows:\n",
    "\n",
    "```dotenv\n",
    "AWS_ACCESS_KEY_ID='<Your Backblaze application key ID>'\n",
    "AWS_SECRET_ACCESS_KEY='<Your Backblaze application key>'\n",
    "AWS_ENDPOINT_URL='<Your bucket endpoint, prefixed with https://, e.g., https://s3.us-west-004.backblazeb2.com >'\n",
    "```\n",
    "\n",
    "When you're done, `.env` should look like this:\n",
    "\n",
    "```dotenv\n",
    "AWS_ACCESS_KEY_ID='004qlekmvpwemrt000000009e'\n",
    "AWS_SECRET_ACCESS_KEY='K004JEKEUTGLKEJFKLRJHTKLVCNWURM'\n",
    "AWS_ENDPOINT_URL='https://s3.us-west-004.backblazeb2.com'\n",
    "```\n",
    "\n",
    "Now you can load the configuration into the environment:"
   ],
   "id": "6b3461adad990f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:35.794639Z",
     "start_time": "2024-07-29T23:49:35.785512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print('Loaded environment variables from .env')\n",
    "else:\n",
    "    print('No environment variables in .env!')"
   ],
   "id": "f40daa9538a70cdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the bucket name to match the bucket you are using",
   "id": "fd4d350b54c5d2e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:35.798709Z",
     "start_time": "2024-07-29T23:49:35.796984Z"
    }
   },
   "cell_type": "code",
   "source": "bucket_name = 'metadaddy-langchain-demo'",
   "id": "5365fb89b9306940",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the PDF location to the prefix (folder/directory) within the bucket that you are using for your PDFs. You can set it to `''` if you put the PDFs in the root of the bucket.",
   "id": "9cb43a12971329e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:39.443281Z",
     "start_time": "2024-07-29T23:49:39.441414Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_location = 'pdfs/cloud_storage'",
   "id": "6cff26ce71b85acc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You'll load data extracted from the PDFs into a vector store, stored in this directory: ",
   "id": "63505d874191aecd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:40.973220Z",
     "start_time": "2024-07-29T23:49:40.971241Z"
    }
   },
   "cell_type": "code",
   "source": "vector_db_directory = 'vectordb'",
   "id": "4ec489bf4db3916c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As part of the tutorial, you'll be archiving the vector store to Backblaze B2, so you need to define the archive's location in your bucket: ",
   "id": "d54ae3e3b1674ef6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:42.268888Z",
     "start_time": "2024-07-29T23:49:42.267005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vector store archive location within your Backblaze B2 bucket\n",
    "vector_db_archive = 'vectordb/vectordb.zip'\n",
    "\n",
    "# S3FileSystem uses the bucket name in the path\n",
    "vector_db_path = f'{bucket_name}/{vector_db_archive}'"
   ],
   "id": "30616e236e7401a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## List the PDF Files for Processing\n",
    "\n",
    "Use Boto3 to list the files in `pdf_location`."
   ],
   "id": "2b6634bc334e4d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:44.321461Z",
     "start_time": "2024-07-29T23:49:43.735630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "\n",
    "b2_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Note - list_object_v2 returns a maximum of 1000 objects per call, \n",
    "    # so you should use a paginator in a real-world implementation. \n",
    "    # See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html\n",
    "    object_list = b2_client.list_objects_v2(Bucket=bucket_name, Prefix=pdf_location)\n",
    "    print(f'Successfully accessed {bucket_name}, found {object_list[\"KeyCount\"]} file(s) under {pdf_location}/')\n",
    "except Exception as e:\n",
    "    print(f'Error accessing B2: {e}')"
   ],
   "id": "c13a594c92b6c3c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed metadaddy-langchain-demo, found 226 file(s) under pdfs/cloud_storage/\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load PDF Data from Backblaze B2\n",
    "\n",
    "Now you can iterate through the list of files, loading each with [`S3FileLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html).\n",
    "\n",
    "This can take a few minutes, depending on how much data you are loading. Most of the time is consumed by parsing the PDF, rather than downloading the data. Note that you need only download and parse the PDF data once. In later steps you'll see how the PDF data is [loaded into a vector store](#Create-a-Vector-Store) that can be persisted to local disk, then [archived to Backblaze B2](#Archive-the-Vector-Store-to-Backblaze-B2) and [downloaded from Backblaze B2](#Download-the-Vector-Store-from-Backblaze-B2) for use with the model."
   ],
   "id": "78f98756782fa6f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:52:36.584898Z",
     "start_time": "2024-07-29T23:50:04.822392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import S3FileLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "print(f'Loading PDF data from B2 bucket {bucket_name}/{pdf_location}')\n",
    "docs = []\n",
    "for object in object_list['Contents']:\n",
    "    # Only process PDF files\n",
    "    if fnmatch(object['Key'], '*.pdf'):\n",
    "        print(f'Loading {object[\"Key\"]}')\n",
    "        loader = S3FileLoader(bucket_name, object['Key'])\n",
    "        docs += loader.load()\n",
    "\n",
    "print(f'Loaded {len(docs)} document(s)')"
   ],
   "id": "dec111a6ce209732",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF data from B2 bucket metadaddy-langchain-demo/pdfs/cloud_storage\n",
      "Loading pdfs/cloud_storage/cloud-storage-add-file-information-with-the-native-api.pdf\n",
      "Loading pdfs/cloud_storage/cloud-storage-api-operations.pdf\n",
      "Loading pdfs/cloud_storage/cloud-storage-application-key-capabilities.pdf\n",
      "...\n",
      "Loading pdfs/cloud_storage/cloud-storage-use-the-start-file-name-parameter-in-the-native-api.pdf\n",
      "Loading pdfs/cloud_storage/cloud-storage-use-tiger-bridge-to-integrate-veeam-backup-to-backblaze-b2.pdf\n",
      "Loading pdfs/cloud_storage/cloud-storage-view-and-delete-unfinished-large-files.pdf\n",
      "Loaded 225 document(s)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You must split the text into chunks for loading into a [vector store](https://python.langchain.com/v0.2/docs/concepts/#vector-stores). A chunk size of 1000 characters, with a 200 character overlap seems to work well for technical articles. You can experiment by changing these parameters.",
   "id": "5370990ac8f5ef04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:31:58.059938Z",
     "start_time": "2024-07-30T00:31:58.015028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f'Split {len(docs)} document(s) into {len(all_splits)} chunks')"
   ],
   "id": "40282d6452f20412",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 225 document(s) into 1594 chunks\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a Vector Store\n",
    "\n",
    "Now create a vector store from the splits.\n",
    "\n",
    "Note that you must specify `persist_directory` so that the vector store will be saved to local disk. "
   ],
   "id": "324c17a74a451aa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:32:48.893294Z",
     "start_time": "2024-07-30T00:32:09.938129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=GPT4AllEmbeddings(model_name='all-MiniLM-L6-v2.gguf2.f16.gguf'),\n",
    "    persist_directory=vector_db_directory\n",
    ")"
   ],
   "id": "46ee410379fd12a8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Running a similarity search on the vector store with a relevant query should return one or more results.",
   "id": "c93ae413bd488ebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:50:14.470520Z",
     "start_time": "2024-07-30T00:50:14.402684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_results = vectorstore.similarity_search('When would you use a master application key?')\n",
    "print(f'Found {len(search_results)} docs')\n",
    "print(f'First doc ({len(search_results[0].page_content)} characters): {search_results[0]}')"
   ],
   "id": "b2dad6006ff9e9ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 docs\n",
      "First doc (237 characters): page_content='a standardapplication key that is limited to the level of access that a user needs. The following use casesdemonstrate when to use a standard app key versus a master application key:Use a standard application key for the following cases:' metadata={'source': 's3://metadaddy-langchain-demo/pdfs/documentation.pdf'}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Archive the Vector Store to Backblaze B2\n",
    "\n",
    "Since it can take considerable time to download, parse, and split the PDF data, then load it into the vector store, it is worth archiving the vector store files for later use.\n",
    "\n",
    "Notice that this code block zips the vector store files directly into a file in Backblaze B2, rather than creating, uploading, then deleting a local ZIP file. This is a useful technique to save time and local storage space!"
   ],
   "id": "3dda06ccb82f2bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:50:56.581578Z",
     "start_time": "2024-07-30T00:50:32.211572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from shutil import copyfileobj\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from s3fs import S3FileSystem\n",
    "from zipfile import ZipFile, ZipInfo\n",
    "\n",
    "b2fs = S3FileSystem(version_aware=True)\n",
    "\n",
    "with b2fs.open(vector_db_path, mode='wb') as f, ZipFile(f, mode='w') as zipfile:\n",
    "    for root, _dirnames, filenames in os.walk(vector_db_directory):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(root, filename)\n",
    "            mtime = os.path.getmtime(fullpath)\n",
    "            last_modified = datetime.fromtimestamp(mtime)\n",
    "            date_time = (last_modified.year, last_modified.month, last_modified.day,\n",
    "                         last_modified.hour, last_modified.minute, last_modified.second)\n",
    "            # Want path relative to vector_db_directory\n",
    "            zipinfo = ZipInfo(filename=fullpath.removeprefix(f'{vector_db_directory}/'), date_time=date_time)\n",
    "            with open(fullpath, mode='rb') as src, zipfile.open(zipinfo, mode='w') as dst:\n",
    "                copyfileobj(src, dst)\n",
    "                print(f'Added {zipinfo.filename}')\n",
    "\n",
    "# Check that the ZIP is there, and see how big it is\n",
    "response = b2fs.ls(vector_db_path, detail=True)\n",
    "print(f'\\nArchived vector store at {vector_db_directory} to {vector_db_archive}; archive size is {response[0][\"size\"]}')"
   ],
   "id": "449930921be3a0aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chroma.sqlite3\n",
      "Added a91c0bf9-2a78-40a4-8ba6-d08665df5027/data_level0.bin\n",
      "Added a91c0bf9-2a78-40a4-8ba6-d08665df5027/length.bin\n",
      "Added a91c0bf9-2a78-40a4-8ba6-d08665df5027/link_lists.bin\n",
      "Added a91c0bf9-2a78-40a4-8ba6-d08665df5027/header.bin\n",
      "Added a91c0bf9-2a78-40a4-8ba6-d08665df5027/index_metadata.pickle\n",
      "\n",
      "Archived vector store at vectordb to vectordb/vectordb.zip; archive size is 54812978\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download the Vector Store from Backblaze B2\n",
    "\n",
    "Once you've archived the vector store to Backblaze B2, for future use, you can download and extract it rather than recreating it from the PDF data.\n",
    "\n",
    "You don't need to do this if you already have a local vector store, but, as a test, you can delete the local vector store before downloading and unzipping the archive by uncommenting the `rmtree()` call."
   ],
   "id": "fb0ce00807c406b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:05.385012Z",
     "start_time": "2024-07-30T00:51:00.711275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from shutil import rmtree\n",
    "\n",
    "# Uncomment the following line if you want to delete the local vector store before you download and extract the archive\n",
    "# rmtree(vector_db_directory)\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(vector_db_directory):\n",
    "    os.mkdir(vector_db_directory)\n",
    "\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "b2fs = S3FileSystem(version_aware=True)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print(f'Downloading and unzipping {vector_db_path} to {vector_db_directory}')\n",
    "with b2fs.open(vector_db_path, mode='rb') as f, ZipFile(f, mode='r') as myzip:\n",
    "    myzip.extractall(vector_db_directory)\n",
    "\n",
    "print('Downloaded and extracted vector store from B2:')\n",
    "for root, _dirnames, filenames in os.walk(vector_db_directory):\n",
    "    for filename in filenames:\n",
    "        fullpath = os.path.join(root, filename)\n",
    "        info = os.stat(fullpath)\n",
    "        print(f'{info.st_size:>12} {fullpath}')"
   ],
   "id": "e267b33f6cd5178a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping metadaddy-langchain-demo/vectordb/vectordb.zip to vectordb\n",
      "Downloaded and extracted vector store from B2:\n",
      "    46080000 vectordb/chroma.sqlite3\n",
      "     8380000 vectordb/a91c0bf9-2a78-40a4-8ba6-d08665df5027/data_level0.bin\n",
      "       20000 vectordb/a91c0bf9-2a78-40a4-8ba6-d08665df5027/length.bin\n",
      "       43732 vectordb/a91c0bf9-2a78-40a4-8ba6-d08665df5027/link_lists.bin\n",
      "         100 vectordb/a91c0bf9-2a78-40a4-8ba6-d08665df5027/header.bin\n",
      "      288034 vectordb/a91c0bf9-2a78-40a4-8ba6-d08665df5027/index_metadata.pickle\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that you've extracted the vector store to its local direction, you can instantiate a vector store object:",
   "id": "8acc19d5f33fff7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:17.977479Z",
     "start_time": "2024-07-30T00:51:17.312759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "# Create the vector store object using the constructor to load it from the local directory\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=GPT4AllEmbeddings(model_name='all-MiniLM-L6-v2.gguf2.f16.gguf'),\n",
    "    persist_directory=\"./vectordb\"\n",
    ")"
   ],
   "id": "dc6fc38b8c32c579",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the Large Language Model (LLM)\n",
    "\n",
    "[GPT4All](https://docs.gpt4all.io/) allows you to run LLMs locally on consumer-grade hardware; it's a great tool for getting started building LLM-based applications.\n",
    "You can [download the GPT4All app](https://www.nomic.ai/gpt4all) and use it to download one or more models, or download model files from [Hugging Face](https://huggingface.co/) directly. GPT4All offers a [wide choice of models](https://docs.gpt4all.io/gpt4all_desktop/models.html); this tutorial uses [Nous Hermes 2 Mistral DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO), a fast chat-based model.\n",
    " \n",
    "If you use the app, you will need to locate the directory to which it downloads models. The location on my Mac is shown below as an example."
   ],
   "id": "2600304da45a6be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:23.032560Z",
     "start_time": "2024-07-30T00:51:22.282747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# Change this to point to the model file on your machine\n",
    "model_path = '/Users/ppatterson/Library/Application Support/nomic.ai/GPT4All/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf'\n",
    "\n",
    "# The device on which to run the model: 'cpu', 'gpu', 'nvidia', 'intel', 'amd' or a DeviceName\n",
    "device = 'gpu'\n",
    "\n",
    "# Maximum size of context window, in tokens. A higher number can produce better responses, but will consume more memory.\n",
    "max_context_window = 4096\n",
    "\n",
    "print(f'Loading LLM, requesting device {device}')\n",
    "model = GPT4All(\n",
    "    model=model_path,\n",
    "    max_tokens=max_context_window,\n",
    "    device=device\n",
    ")\n",
    "print(f'Loaded LLM, running on {model.device}.')"
   ],
   "id": "235b86c6eccdec94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM, requesting device gpu\n",
      "Loaded LLM, running on gpu.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As its name implies, LangChain allows you to combine components such as vector stores and LLMs into chains to implement a wide variety of use cases. Each component in the chain accepts input, performs some processing, and emits some output. ",
   "id": "fd368d160a4c1e97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Get a Retriever to Use in the Chain \n",
    "\n",
    "To use a vector store in a chain, you obtain its `retriever` interface - the retriever accepts string queries and returns the most 'relevant' documents from its source."
   ],
   "id": "65d9c93974bd251f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:26.550293Z",
     "start_time": "2024-07-30T00:51:26.548319Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vectorstore.as_retriever()",
   "id": "d272f1bbff2d7ede",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define a Prompt Template\n",
    "\n",
    "You need to define a prompt template to frame the interaction with the LLM. In this RAG chain, it will combine instructions, the context retrieved from the vector store, and the user's question.\n",
    "\n",
    "This prompt template is based on the example Q&A RAQ prompt at\n",
    "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/retrieval_qa/prompt.py. Note how it explicitly instructs the model to use the provided context in answering the question, and not to try to make up an answer. `{context}` and `{question}` are placeholders; the relevant text will be substituted as the chain executes."
   ],
   "id": "e040074b23fa0673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:28.569192Z",
     "start_time": "2024-07-30T00:51:28.530217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ],
   "id": "a6860e60afaa16e1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build a chain\n",
    "\n",
    "Now you have all the ingredients to build a chain! You can see how the context and question are fed into the prompt, the result being fed into the model, the output of which is fed into a `StrOutputParser()` to produce a string."
   ],
   "id": "a54392f78a7a8b8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:51:32.645186Z",
     "start_time": "2024-07-30T00:51:32.642536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "id": "1f13bcb0d3b81fcb",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's feed a few questions through the chain. Note that this chain does not implement chat history, so each question must be self-contained. Feel free to edit the questions and see if you can stump the chatbot!",
   "id": "5f5567b394204499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:52:23.234345Z",
     "start_time": "2024-07-30T00:51:38.892472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    'What is the difference between the master application key and a standard application key?',\n",
    "    'What are best practices for working with application keys?',\n",
    "    'Tell me about event notifications in Backblaze B2'\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f'\\n{question}\\n')\n",
    "    answer = chain.invoke(question)\n",
    "    print(f'{answer}\\n')"
   ],
   "id": "674640ef84cef8dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the difference between the master application key and a standard application key?\n",
      "\n",
      " A master application key grants broad access privileges, while a standard application key limits access to only what a user needs.\n",
      "\n",
      "\n",
      "What are best practices for working with application keys?\n",
      "\n",
      " Use a standard application key that is limited to the level of access that a user needs, and avoid using a master application key unless necessary due to its broad access privileges.\n",
      "\n",
      "\n",
      "Tell me about event notifications in Backblaze B2\n",
      "\n",
      " Event Notifications are used in Backblaze B2 to notify users of certain events, such as file deletions or changes. They can be managed using the Backblaze B2 API and can trigger actions when an event occurs. Each message contains a signature that is sent in the x-bz-event-notification-signature HTTP header to ensure its authenticity and prevent tampering.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding Conversation History\n",
    "\n",
    "You can ask the chatbot questions, but this isn't really a conversation–you can't refer back to earlier questions and answers. In this section, you'll use LangChain's [`RunnableWithMessageHistory`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) class to manage chat message history for an existing chain.\n",
    "\n",
    "First, you need to redefine `prompt_template` to include the history:"
   ],
   "id": "96669f7e4e75a1b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:54:16.819201Z",
     "start_time": "2024-07-30T00:54:16.813859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context and the message history to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    History: {history}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\", \"history\"]\n",
    ")"
   ],
   "id": "ec806029505759b9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The existing chain does not meet the requirements for `RunnableWithMessageHistory`:\n",
    "\n",
    "> Must take as input one of: 1. A sequence of BaseMessages 2. A dict with one key for all messages 3. A dict with one key for the current input string/message(s) and a separate key for historical messages.\n",
    "\n",
    "You can redefine the chain so that its input meets the third option: a dict with one key for the current input string/message(s) and a separate key for historical messages:"
   ],
   "id": "f92ed7b9e6a4a0e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:54:21.460813Z",
     "start_time": "2024-07-30T00:54:21.457651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": (\n",
    "                itemgetter(\"question\")\n",
    "                | retriever\n",
    "        ),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"history\": itemgetter(\"history\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "6fbc523ca73afc49",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The previous chain's input was simply a string containing the question. This chain accepts a dict with keys `question` and `history`. The first step of the chain passes the question to the retriever to obtain the context for the prompt and simply passes the question and history on, emitting a dict with keys `context`, `question` and `history` for consumption by the prompt.\n",
    "\n",
    "The message history must be stored between interactions. For this tutorial, a simple in-memory message store suffices, but in a real-world use case you might use a message history class that is backed by a persistent store such as [`RedisChatMessageHistory`](https://api.python.langchain.com/en/latest/chat_message_histories/langchain_community.chat_message_histories.redis.RedisChatMessageHistory.html)."
   ],
   "id": "e0c0e0e5746fbd99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:54:30.234601Z",
     "start_time": "2024-07-30T00:54:30.232042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ],
   "id": "b43ba8a5dccb7392",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This simple implementation uses a session ID so that the store can support multiple users, each with a different session ID.\n",
    "\n",
    "Now you can use `RunnableWithMessageHistory` to wrap the chain with the message history:"
   ],
   "id": "2e90bf2bf8da33f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:54:32.305375Z",
     "start_time": "2024-07-30T00:54:32.299588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ],
   "id": "70b005074f789afa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can ask a series of related questions, and the chatbot will use the conversation history in constructing its replies.",
   "id": "e782ea980cb65f1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:55:26.000473Z",
     "start_time": "2024-07-30T00:54:33.859074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    'What is the difference between the master application key and a standard application key?',\n",
    "    'Which one would I use to work with a single bucket?',\n",
    "    'Can you tell me anything more about this topic?'\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f'\\n{question}\\n')\n",
    "    answer = with_message_history.invoke(\n",
    "        {\"question\": question},\n",
    "        config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "    )\n",
    "    print(f'{answer}\\n')"
   ],
   "id": "c940a929d88ec8c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the difference between the master application key and a standard application key?\n",
      "\n",
      " A master application key grants broad access privileges, while a standard application key is limited to the level of access that a user needs.\n",
      "\n",
      "\n",
      "Which one would I use to work with a single bucket?\n",
      "\n",
      " You would use a standard application key to work with a single bucket as it has limited access and only grants permissions needed for specific tasks, unlike the master application key which provides broad access privileges.\n",
      "\n",
      "\n",
      "Can you tell me anything more about this topic?\n",
      "\n",
      " Sure! The master application key is typically used by developers during development or testing phases to grant full access to all resources in a Backblaze B2 account, while the standard application key provides limited permissions and should be used for production environments where security is paramount.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's pretty good - the chatbot is clearly using the message history.",
   "id": "9093813648f34534"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations - you have a conversational chatbot that answers questions based on context you provided!\n",
    "\n",
    "Try experimenting with chunk size, overlap, and the maximum context window and observe how the model behaves. You can even swap out the model–GPT4All supports a [range of alternative models](https://docs.gpt4all.io/gpt4all_desktop/models.html), or you can use a different model framework entirely."
   ],
   "id": "c689b8891ae5b0ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f51e1d23c48f9e3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
