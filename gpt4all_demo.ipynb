{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GPT4All Demo\n",
    "This notebook shows how to implement a chat session with [GPT4All](https://www.nomic.ai/gpt4all).\n",
    "\n",
    "First, install the Python dependencies. You can do this from the notebook, or from the command-line before you start Jupyter."
   ],
   "id": "8d9a7ea0d9a358c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:45:09.575363Z",
     "start_time": "2024-07-17T01:45:06.603297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --quiet -r requirements.txt\n",
    "\n",
    "# Restart the kernel so that it uses the new modules\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ],
   "id": "64c01372b6f5a26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "You can [download the GPT4All app](https://www.nomic.ai/gpt4all) and use it to download one or more models, or download model files from [Hugging Face](https://huggingface.co/) directly. If you use the app, you will need to locate the directory to which it downloads models. The location on my Mac is shown below as an example."
   ],
   "id": "fa2aa2df196d27d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:41.379894Z",
     "start_time": "2024-07-16T21:53:41.376299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Name of a model file compatible with GPT4All\n",
    "model_name = 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf'\n",
    "\n",
    "# Location of the model file\n",
    "model_directory = '/Users/ppatterson/Library/Application Support/nomic.ai/GPT4All'\n",
    "\n",
    "# Maximum size of context window, in tokens. A higher number can produce better responses, but will consume more memory.\n",
    "max_context_window = 4096\n",
    "\n",
    "# The device on which to run the model. One of the following values:\n",
    "# 'cpu': Model will run on the central processing unit.\n",
    "# 'gpu': Use Metal on ARM64 macOS, otherwise the same as \"kompute\".\n",
    "# \"kompute\": Use the best GPU provided by the Kompute backend.\n",
    "# \"cuda\": Use the best GPU provided by the CUDA backend.\n",
    "# \"amd\", \"nvidia\": Use the best GPU provided by the Kompute backend from this vendor.\n",
    "device = 'gpu'\n",
    "\n",
    "print(f'\\nUsing {model_name} from {model_directory}, with a maximum context window of {max_context_window} tokens. Requesting the {device} device')"
   ],
   "id": "90c6caaf0227222b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf from /Users/ppatterson/Library/Application Support/nomic.ai/GPT4All, with a maximum context window of 4096 tokens. Requesting the gpu device\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the Large Language Model (LLM)\n",
    "\n",
    "Note that GPT4All may not be able to use the requested device, and may not report it correctly even if it does! "
   ],
   "id": "f7ffe1982988f3cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:42.387406Z",
     "start_time": "2024-07-16T21:53:41.381034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "print(f'Loading LLM, requesting device {device}')\n",
    "\n",
    "model = GPT4All(\n",
    "    model_name=model_name,\n",
    "    model_path=model_directory,\n",
    "    device=device,\n",
    "    n_ctx=int(max_context_window)\n",
    ")\n",
    "\n",
    "# GPT4All may return None as the device!\n",
    "print(f'Loaded LLM, running on {model.device if model.device else \"cpu?\"}, using the {model.backend} backend.')"
   ],
   "id": "417721a89273a894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM, requesting device gpu\n",
      "Loaded LLM, running on cpu?, using the metal backend.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configure a Prompt Template\n",
    "\n",
    "These are the basic instructions for the LLM. Feel free to experiment by changing the prompt!"
   ],
   "id": "2dedf11321069147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:42.391720Z",
     "start_time": "2024-07-16T21:53:42.389505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = \"\"\"Briefly answer the question below.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "### Question: {0}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ],
   "id": "9bba697b29f1ff93",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process a Series of Questions within a Chat Session\n",
    "\n",
    "GPT4All's [`chat_session`](https://docs.gpt4all.io/gpt4all_python/ref.html#gpt4all.gpt4all.GPT4All.chat_session) context manager allows the model to retain context from previous interactions when answering questions. "
   ],
   "id": "cc144b9ec6de8acd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:55:12.717934Z",
     "start_time": "2024-07-16T21:55:02.754958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    'When was the Battle of Hastings?',\n",
    "    'Who won it?',\n",
    "    'What did they do next?'\n",
    "]\n",
    "\n",
    "with model.chat_session(prompt_template=prompt_template):\n",
    "    for question in questions:\n",
    "        answer = model.generate(question)\n",
    "        print(f'\\n{question}\\n{answer}\\n')"
   ],
   "id": "444831ac587d12f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When was the Battle of Hastings?\n",
      "The Battle of Hastings took place on October 14, 1066. It is considered one of the most significant battles in English history as it led to the Norman Conquest of England by William the Conqueror.\n",
      "\n",
      "\n",
      "Who won it?\n",
      "The Battle of Hastings was a decisive victory for William the Conqueror and his Norman forces over King Harold II and his English army. This battle marked the beginning of the Norman conquest of England, with William eventually being crowned as the first Norman king of England in 1066.\n",
      "\n",
      "\n",
      "What did they do next?\n",
      "After winning the Battle of Hastings, William the Conqueror continued his conquest of England by marching towards London where he was eventually recognized as king after a standoff at the city gates. He then set about consolidating his rule and implementing significant changes in governance, law, language, culture, and religion that would shape England for centuries to come.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Contrast with the same series of questions outside the context of a Chat Session. The second and third responses are somewhat random!",
   "id": "1deabfe48b68f4fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:55:37.683075Z",
     "start_time": "2024-07-16T21:55:30.429123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for question in questions:\n",
    "    # Use format to combine the prompt template with the question  \n",
    "    answer = model.generate(prompt_template.format(question))\n",
    "    print(f'\\n{question}\\n{answer}\\n')"
   ],
   "id": "bd890fc09660409f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When was the Battle of Hastings?\n",
      "The Battle of Hastings took place on October 14th, 1066. It is considered one of the most significant battles in English history as it led to the Norman Conquest of England by William the Conqueror.\n",
      "\n",
      "\n",
      "Who won it?\n",
      "The winner of the 2019 Women's World Cup was the United States women's national soccer team. They defeated the Netherlands in a final match with a score of 2-0, winning their fourth title overall and second consecutive championship.\n",
      "\n",
      "\n",
      "What did they do next?\n",
      "They went outside and played in the park.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ca94e986a12cd30e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
