{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Backblaze B2 Retrieval-Augmented Generation (RAG) Demo\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) allows you to build on an off-the-shelf large language model (LLM), adding custom context that the model can use in interacting with a user. You can use RAG to implement chatbots that use your own proprietary data to answer questions, without that data leaking to the internet. \n",
    "\n",
    "This notebook walks you through loading PDF files from [Backblaze B2 Cloud Object Storage](https://www.backblaze.com/cloud-storage) into a [LangChain](https://python.langchain.com/v0.2/docs/introduction/) RAG app, then building a chatbot that can answer questions relating to the content of those PDF files. You'll use an open-source language model that you run locally, rather than an online API, ensuring that your data stays confidential.\n",
    "\n",
    "The code is based on the LangChain tutorial, [Build a Local RAG Application](https://python.langchain.com/v0.2/docs/tutorials/local_rag/).\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "First, install the required Python packages:"
   ],
   "id": "75a51ec771ee51d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T22:08:50.711320Z",
     "start_time": "2024-07-16T22:08:48.016994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --quiet -r requirements.txt\n",
    "\n",
    "# Restart the kernel so that it uses the new modules\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ],
   "id": "57a3b3958767b924",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need a Backblaze B2 Account, Bucket and Application Key, and some PDF files. Follow these instructions, as necessary:\n",
    "\n",
    "* [Create a Backblaze B2 Account](https://www.backblaze.com/sign-up/cloud-storage).\n",
    "* [Create a Backblaze B2 Bucket](https://www.backblaze.com/docs/cloud-storage-create-and-manage-buckets).\n",
    "* [Create an Application Key](https://www.backblaze.com/docs/cloud-storage-create-and-manage-app-keys#create-an-app-key) with access to the bucket you wish to use.\n",
    "\n",
    "Be sure to copy the application key as soon as you create it, as you will not be able to retrieve it later!"
   ],
   "id": "b401105aa7ab8e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upload PDF Files to Your Bucket\n",
    "\n",
    "You can use the Backblaze web UI, or any B2 or S3-compatible file management to [upload PDF files to your bucket](https://www.backblaze.com/docs/cloud-storage-upload-and-manage-files). It's useful to organize files by prefix (analogous to a folder or directory in a traditional filesystem); this example assumes the PDFs have the prefix `pdfs/` within the bucket.\n",
    "\n",
    "If you don't have any suitable PDF files to hand, you can [download the Backblaze B2 documentation PDF used in creating this tutorial](https://metadaddy-langchain-demo.s3.us-west-004.backblazeb2.com/pdfs/documentation.pdf) and upload it to your own bucket. "
   ],
   "id": "9a708475a5b58ccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Since Backblaze B2 has an S3-compatible API, this notebook uses LangChain's [`S3FileLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html) and the [`s3fs`](https://s3fs.readthedocs.io/en/latest/) module to interact with files in Backblaze B2, as well as the AWS SDK for Python, also known as Boto3. Both `S3FileLoader` and `s3fs` use Boto3 under the covers, so you need simply configure the latter so that all of the tools can access your Backblaze B2 Bucket. The most straightforward way to do so in this context is via environment variables.\n",
    "\n",
    "Note: you should never, *ever* put credentials in your code, including Jupyter notebooks! This example uses `python-dotenv` to load configuration from a `.env` file into environment variables for use by `S3FileLoader`. This repo includes a template file, `.env.template`. Copy it to `.env`, then edit it as follows:\n",
    "\n",
    "```dotenv\n",
    "AWS_ACCESS_KEY_ID='<Your Backblaze application key ID>'\n",
    "AWS_SECRET_ACCESS_KEY='<Your Backblaze application key>'\n",
    "AWS_ENDPOINT_URL='<Your bucket endpoint, prefixed with https://, e.g., https://s3.us-west-004.backblazeb2.com >'\n",
    "```\n",
    "\n",
    "When you're done, `.env` should look like this:\n",
    "\n",
    "```dotenv\n",
    "AWS_ACCESS_KEY_ID='004qlekmvpwemrt000000009e'\n",
    "AWS_SECRET_ACCESS_KEY='K004JEKEUTGLKEJFKLRJHTKLVCNWURM'\n",
    "AWS_ENDPOINT_URL='https://s3.us-west-004.backblazeb2.com'\n",
    "```\n",
    "\n",
    "Now you can load the configuration into the environment:"
   ],
   "id": "6b3461adad990f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T16:54:24.904088Z",
     "start_time": "2024-07-17T16:54:24.895748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print('Loaded environment variables from .env')\n",
    "else:\n",
    "    print('No environment variables in .env!')"
   ],
   "id": "f40daa9538a70cdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the bucket name to match the bucket you are using",
   "id": "fd4d350b54c5d2e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T17:01:15.360777Z",
     "start_time": "2024-07-17T17:01:15.358293Z"
    }
   },
   "cell_type": "code",
   "source": "bucket_name = 'metadaddy-langchain-demo'",
   "id": "5365fb89b9306940",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the PDF location to the prefix (folder/directory) within the bucket that you are using for your PDFs. You can set it to `''` if you put the PDFs in the root of the bucket.",
   "id": "9cb43a12971329e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T17:01:11.405780Z",
     "start_time": "2024-07-17T17:01:11.400492Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_location = 'pdfs'",
   "id": "6cff26ce71b85acc",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## List the PDF Files for Processing\n",
    "\n",
    "Use Boto3 to list the files in `pdf_location`."
   ],
   "id": "2b6634bc334e4d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:46:11.785031Z",
     "start_time": "2024-07-17T01:46:11.609979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "\n",
    "b2_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Note - list_object_v2 returns a maximum of 1000 objects per call, \n",
    "    # so you should use a paginator in a real-world implementation. \n",
    "    # See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html\n",
    "    object_list = b2_client.list_objects_v2(Bucket=bucket_name, Prefix=pdf_location)\n",
    "    print(f'Successfully accessed {bucket_name}, found {object_list[\"KeyCount\"]} file(s) under {pdf_location}/')\n",
    "except Exception as e:\n",
    "    print(f'Error accessing B2: {e}')"
   ],
   "id": "c13a594c92b6c3c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed metadaddy-langchain-demo, found 1 file(s) under pdfs/\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load PDF Data from Backblaze B2\n",
    "\n",
    "Now you can iterate through the list of files, loading each with [`S3FileLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html).\n",
    "\n",
    "This can take a few minutes, depending on how much data you are loading. Most of the time is consumed by parsing the PDF, rather than downloading the data. Note that you need only download and parse the PDF data once. In later steps you'll see how the PDF data is [loaded into a vector store](#Create-a-Vector-Store) that can be persisted to local disk, then [archived to Backblaze B2](#Archive-the-Vector-Store-to-Backblaze-B2) and [downloaded from Backblaze B2](#Download-the-Vector-Store-from-Backblaze-B2) for use with the model."
   ],
   "id": "78f98756782fa6f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:00:35.483736Z",
     "start_time": "2024-07-16T22:58:55.987714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import S3FileLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "print(f'Loading PDF data from B2 bucket {bucket_name}/{pdf_location}')\n",
    "docs = []\n",
    "for object in object_list['Contents']:\n",
    "    # Only process PDF files\n",
    "    if fnmatch(object['Key'], '*.pdf'):\n",
    "        print(f'Loading {object[\"Key\"]}')\n",
    "        loader = S3FileLoader(bucket_name, object['Key'])\n",
    "        docs += loader.load()\n",
    "\n",
    "print(f'Loaded {len(docs)} document(s)')"
   ],
   "id": "dec111a6ce209732",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF data from B2 bucket metadaddy-langchain-demo/pdfs\n",
      "Loading pdfs/documentation.pdf\n",
      "Loaded 1 document(s)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You must split the text into chunks for loading into a [vector store](https://python.langchain.com/v0.2/docs/concepts/#vector-stores). A chunk size of 1000 characters, with a 200 character overlap seems to work well for technical articles. You can experiment by changing these parameters.",
   "id": "5370990ac8f5ef04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:01:34.454430Z",
     "start_time": "2024-07-16T23:01:34.309005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f'Split {len(docs)} document(s) into {len(all_splits)} chunks')"
   ],
   "id": "40282d6452f20412",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 document(s) into 1982 chunks\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a Vector Store\n",
    "\n",
    "Now create a vector store from the splits.\n",
    "\n",
    "Note that we specify `persist_directory` so that the vector store will be saved to local disk. "
   ],
   "id": "324c17a74a451aa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:39:59.880336Z",
     "start_time": "2024-07-16T23:39:26.051689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "# The vector store will be persisted into this local directory\n",
    "vector_db_directory = 'vectordb'\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=GPT4AllEmbeddings(model_name='all-MiniLM-L6-v2.gguf2.f16.gguf', gpt4all_kwargs={'allow_download': True}),\n",
    "    persist_directory=vector_db_directory\n",
    ")"
   ],
   "id": "46ee410379fd12a8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Running a similarity search on the vector store with a relevant query should return one or more results.",
   "id": "c93ae413bd488ebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:40:02.281103Z",
     "start_time": "2024-07-16T23:40:02.250746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_results = vectorstore.similarity_search('When would you use a master application key?')\n",
    "print(f'Found {len(search_results)} docs')\n",
    "print(f'First doc ({len(search_results[0].page_content)} characters): {search_results[0]}')"
   ],
   "id": "b2dad6006ff9e9ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 docs\n",
      "First doc (237 characters): page_content='a standardapplication key that is limited to the level of access that a user needs. The following use casesdemonstrate when to use a standard app key versus a master application key:Use a standard application key for the following cases:' metadata={'source': 's3://metadaddy-langchain-demo/pdfs/documentation.pdf'}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Archive the Vector Store to Backblaze B2\n",
    "\n",
    "Since it can take considerable time to download, parse, and split the PDF data, then load it into the vector store, it is worth archiving the vector store files for later use.\n",
    "\n",
    "Notice that this code block zips the vector store files directly into a file in Backblaze B2, rather than creating, uploading, then deleting a local ZIP file. This is a useful technique to save time and local storage space!"
   ],
   "id": "3dda06ccb82f2bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:30:19.501489Z",
     "start_time": "2024-07-17T01:30:03.935349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from shutil import copyfileobj\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from s3fs import S3FileSystem\n",
    "from zipfile import ZipFile, ZipInfo\n",
    "\n",
    "b2fs = S3FileSystem(version_aware=True)\n",
    "\n",
    "# Vector store archive location within your Backblaze B2 bucket\n",
    "vector_db_archive = 'vectordb/vectordb.zip'\n",
    "\n",
    "# S3FileSystem uses the bucket name in the path\n",
    "vector_db_path = f'{bucket_name}/{vector_db_archive}'\n",
    "\n",
    "with b2fs.open(vector_db_path, mode='wb') as f, ZipFile(f, mode='w') as zipfile:\n",
    "    for root, _dirnames, filenames in os.walk(vector_db_directory):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(root, filename)\n",
    "            mtime = os.path.getmtime(fullpath)\n",
    "            last_modified = datetime.fromtimestamp(mtime)\n",
    "            date_time = (last_modified.year, last_modified.month, last_modified.day,\n",
    "                         last_modified.hour, last_modified.minute, last_modified.second)\n",
    "            # Want path relative to vector_db_directory\n",
    "            zipinfo = ZipInfo(filename=fullpath.removeprefix(f'{vector_db_directory}/'), date_time=date_time)\n",
    "            with open(fullpath, mode='rb') as src, zipfile.open(zipinfo, mode='w') as dst:\n",
    "                copyfileobj(src, dst)\n",
    "                print(f'Added {zipinfo.filename}')\n",
    "\n",
    "# Check that the ZIP is there, and see how big it is\n",
    "response = b2fs.ls(vector_db_path, detail=True)\n",
    "print(f'\\nArchived vector store at {vector_db_directory} to {vector_db_archive}; archive size is {response[0][\"size\"]}')"
   ],
   "id": "449930921be3a0aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chroma.sqlite3\n",
      "Added e4616987-a3bb-4405-9250-2fdac7275b64/data_level0.bin\n",
      "Added e4616987-a3bb-4405-9250-2fdac7275b64/length.bin\n",
      "Added e4616987-a3bb-4405-9250-2fdac7275b64/link_lists.bin\n",
      "Added e4616987-a3bb-4405-9250-2fdac7275b64/header.bin\n",
      "Added e4616987-a3bb-4405-9250-2fdac7275b64/index_metadata.pickle\n",
      "\n",
      "Archived vector store at vectordb to vectordb/vectordb.zip; archive size is 37986676\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download the Vector Store from Backblaze B2\n",
    "\n",
    "Once you've archived the vector store to Backblaze B2, for future use, you can download and unzip it rather than recreating it from the PDF data.\n",
    "\n",
    "You don't need to do this if you already have a local vector store, but, as a test, you can delete the local vector store before downloading and unzipping the archive by uncommenting the `rmtree()` call."
   ],
   "id": "fb0ce00807c406b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:44:15.372961Z",
     "start_time": "2024-07-17T01:44:11.371201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from shutil import rmtree\n",
    "\n",
    "# Uncomment the following line if you want to delete the local vector store before you download and unzip the archive\n",
    "# rmtree(vector_db_directory)\n",
    "\n",
    "if not os.path.isdir(vector_db_directory):\n",
    "    os.mkdir(vector_db_directory)\n",
    "\n",
    "print(f'Downloading and unzipping {vector_db_path} to {vector_db_directory}')\n",
    "with b2fs.open(vector_db_path, mode='rb') as f, ZipFile(f, mode='r') as myzip:\n",
    "    myzip.extractall(vector_db_directory)\n",
    "\n",
    "print('Downloaded and extracted vector store from B2:')\n",
    "for root, _dirnames, filenames in os.walk(vector_db_directory):\n",
    "    for filename in filenames:\n",
    "        fullpath = os.path.join(root, filename)\n",
    "        info = os.stat(fullpath)\n",
    "        print(f'{info.st_size:>12} {fullpath}')"
   ],
   "id": "dc6fc38b8c32c579",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping metadaddy-langchain-demo/vectordb/vectordb.zip to vectordb\n",
      "Downloaded and extracted vector store from B2:\n",
      "    32747520 vectordb/chroma.sqlite3\n",
      "     5028000 vectordb/e4616987-a3bb-4405-9250-2fdac7275b64/data_level0.bin\n",
      "       12000 vectordb/e4616987-a3bb-4405-9250-2fdac7275b64/length.bin\n",
      "       25940 vectordb/e4616987-a3bb-4405-9250-2fdac7275b64/link_lists.bin\n",
      "         100 vectordb/e4616987-a3bb-4405-9250-2fdac7275b64/header.bin\n",
      "      172004 vectordb/e4616987-a3bb-4405-9250-2fdac7275b64/index_metadata.pickle\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Load the Large Language Model (LLM)\n",
    "\n",
    "[GPT4All](https://docs.gpt4all.io/) allows you to run LLMs locally on consumer-grade hardware; it's a great tool for getting started building LLM-based applications.\n",
    "You can [download the GPT4All app](https://www.nomic.ai/gpt4all) and use it to download one or more models, or download model files from [Hugging Face](https://huggingface.co/) directly. GPT4All offers a [wide choice of models](https://docs.gpt4all.io/gpt4all_desktop/models.html); this tutorial uses [Nous Hermes 2 Mistral DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO), a fast chat-based model.\n",
    " \n",
    "If you use the app, you will need to locate the directory to which it downloads models. The location on my Mac is shown below as an example."
   ],
   "id": "2600304da45a6be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:13:55.764621Z",
     "start_time": "2024-07-16T23:13:54.826380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# Change this to point to the model file on your machine\n",
    "model_path = '/Users/ppatterson/Library/Application Support/nomic.ai/GPT4All/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf'\n",
    "\n",
    "# The device on which to run the model: 'cpu', 'gpu', 'nvidia', 'intel', 'amd' or a DeviceName\n",
    "device = 'gpu'\n",
    "\n",
    "# Maximum size of context window, in tokens. A higher number can produce better responses, but will consume more memory.\n",
    "max_context_window = 4096\n",
    "\n",
    "print(f'Loading LLM, requesting device {device}')\n",
    "model = GPT4All(\n",
    "    model=model_path,\n",
    "    max_tokens=max_context_window,\n",
    "    device=device\n",
    ")\n",
    "print(f'Loaded LLM, running on {model.device}.')"
   ],
   "id": "235b86c6eccdec94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM, requesting device gpu\n",
      "Loaded LLM, running on gpu.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As its name implies, LangChain allows you to combine components such as vector stores and LLMs into chains to implement a wide variety of use cases. Each component in the chain accepts input, performs some processing, and emits some output.\n",
    " \n",
    "To use a vector store in a chain, you obtain its `retriever` interface - the retriever accepts string queries and returns the most 'relevant' documents from its source."
   ],
   "id": "65d9c93974bd251f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:54:01.997109Z",
     "start_time": "2024-07-17T01:54:01.994059Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vectorstore.as_retriever()",
   "id": "d272f1bbff2d7ede",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You need to define a prompt template to frame the interaction with the LLM. In this RAG chain, it will combine instructions, the context retrieved from the vector store, and the user's question.\n",
    "\n",
    "This prompt template is based on the example Q&A RAQ prompt at\n",
    "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/retrieval_qa/prompt.py. Note how it explicitly instructs the model to use the provided context in answering the question, and not to try to make up an answer. `{context}` and `{question}` are placeholders; the relevant text will be substituted as the chain executes."
   ],
   "id": "e040074b23fa0673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T01:54:06.245679Z",
     "start_time": "2024-07-17T01:54:06.209145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ],
   "id": "a6860e60afaa16e1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you have all the ingredients to build a chain! You can see how the context and question are fed into the prompt, the result being fed into the model, the output of which is fed into a `StrOutputParser()` to produce a string.",
   "id": "a54392f78a7a8b8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:22:20.823394Z",
     "start_time": "2024-07-16T23:22:20.820860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "id": "1f13bcb0d3b81fcb",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's feed a few questions through the chain. Note that this chain does not implement chat history, so each question must be self-contained. Feel free to edit the questions and see if you can stump the chatbot!",
   "id": "5f5567b394204499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T23:25:03.897485Z",
     "start_time": "2024-07-16T23:24:09.874102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    'What is the difference between the master application key and a standard application key?',\n",
    "    'What are best practices for working with application keys?',\n",
    "    'Tell me about event notifications in Backblaze B2'\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = chain.invoke(question)\n",
    "    print(f'\\n{question}\\n{answer}\\n')"
   ],
   "id": "674640ef84cef8dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the difference between the master application key and a standard application key?\n",
      " The master application key provides complete access to your Backblaze B2 Cloud Storage account, while a standard application key has specific file prefixes, limited capabilities, can expire, and can be deleted without disrupting other users.\n",
      "\n",
      "\n",
      "What are best practices for working with application keys?\n",
      " Use a standard application key when access is limited to what a user needs; use a master application key only in situations where that level of access is necessary; ensure the key string and ID are securely saved after creation.\n",
      "\n",
      "\n",
      "Tell me about event notifications in Backblaze B2\n",
      " Event Notifications are a feature of Backblaze B2 Cloud Storage that allow users to receive real-time updates on events happening within their storage. These notifications can be set up using rules, which specify the type of event and the action to take when it occurs. The rules also include options for enabling or disabling the notification and naming the rule for identification purposes. Backblaze B2 provides documentation on how to create and manage Event Notification rules as well as guidelines for verifying signatures in notification messages to ensure their authenticity.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations - you have a chatbot that answers questions based on context you provided! Try experimenting with chunk size, overlap, and the maximum context window and observe how the model behaves. You can even swap out the modelâ€“GPT4All supports a [range of alternative models](https://docs.gpt4all.io/gpt4all_desktop/models.html), or you can use a different model framework entirely."
   ],
   "id": "c689b8891ae5b0ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
